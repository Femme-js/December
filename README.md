# December-to-Remember
A repo consisting intresting and mini Machine Learning Projects.

## Dec_01
- GPT-2 is the advanced version of a transformer-based model that was trained to generates synthetic text samples from a variety of user-prompts as input.
- The smaller version which has 117 MB parameters that can be retrained on custom text dataset at the time of this writing.

### Fine-Tuning GPT2

- In essence. fine-tuning in ML is just re-training of the midel with bew data but with existing weights.
- To acess the saved model weight, model definition in a framework of our choice, a lot code to load the model and creating a pipeline.
- In the interest of time and sanity, we already have a beautiful library called gpt-2 simple, which makes this process easy.
- 

